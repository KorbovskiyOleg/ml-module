import pandas as pd
import numpy as np
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
import joblib
import os
from sqlalchemy import create_engine
from dotenv import load_dotenv

# --- –ó–∞–≥—Ä—É–∂–∞–µ–º .env ---
load_dotenv()

DB_USER = os.getenv("DB_USER")
DB_PASS = os.getenv("DB_PASS")
DB_HOST = os.getenv("DB_HOST", "localhost")
DB_PORT = os.getenv("DB_PORT", "5432")
DB_NAME = os.getenv("DB_NAME")
DB_SCHEMA = os.getenv("DB_SCHEMA", "ml_data")

# --- –ü–æ–¥–∫–ª—é—á–∞–µ–º—Å—è –∫ –±–∞–∑–µ ---
engine = create_engine(
    f"postgresql+psycopg2://{DB_USER}:{DB_PASS}@{DB_HOST}:{DB_PORT}/{DB_NAME}",
    connect_args={"options": f"-csearch_path={DB_SCHEMA}"}
)


def analyze_dataset(x, y):
    """–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞"""
    print("\n" + "=" * 60)
    print("üìä –î–ï–¢–ê–õ–¨–ù–´–ô –ê–ù–ê–õ–ò–ó –î–ê–¢–ê–°–ï–¢–ê")
    print("=" * 60)

    print(f"üî¢ –û–±—â–∏–π —Ä–∞–∑–º–µ—Ä –¥–∞–Ω–Ω—ã—Ö: {len(x)} —Å—Ç—Ä–æ–∫")
    print(f"üéØ –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {x.shape[1]}")
    print(f"üè∑Ô∏è  –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {y.nunique()}")
    print(f"üìà –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
    class_distribution = y.value_counts().sort_index()
    for class_label, count in class_distribution.items():
        percentage = (count / len(y)) * 100
        print(f"   –ö–ª–∞—Å—Å {class_label}: {count} samples ({percentage:.1f}%)")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è
    missing_values = x.isnull().sum()
    if missing_values.any():
        print(f"\n‚ö†Ô∏è  –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è:")
        for col, missing in missing_values[missing_values > 0].items():
            print(f"   {col}: {missing} –ø—Ä–æ–ø—É—Å–∫–æ–≤")
    else:
        print(f"\n‚úÖ –ü—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π –Ω–µ—Ç")

    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –¥—É–±–ª–∏–∫–∞—Ç—ã
    duplicates = x.duplicated().sum()
    print(f"\nüîç –î—É–±–ª–∏–∫–∞—Ç—ã –≤ –¥–∞–Ω–Ω—ã—Ö: {duplicates} ({duplicates / len(x) * 100:.1f}%)")

    if duplicates > len(x) * 0.1:  # –ï—Å–ª–∏ –±–æ–ª—å—à–µ 10% –¥—É–±–ª–∏–∫–∞—Ç–æ–≤
        print("‚ö†Ô∏è  –í–ù–ò–ú–ê–ù–ò–ï: –ú–Ω–æ–≥–æ –¥—É–±–ª–∏–∫–∞—Ç–æ–≤! –≠—Ç–æ –º–æ–∂–µ—Ç –≤–ª–∏—è—Ç—å –Ω–∞ –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–∏.")


def compare_models(x, y):
    """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ Decision Tree –∏ Random Forest"""
    x_train, x_test, y_train, y_test = train_test_split(
        x, y, test_size=0.3, random_state=42, stratify=y
    )

    print(f"\nüìä –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö:")
    print(f"   –û–±—É—á–∞—é—â–∞—è –≤—ã–±–æ—Ä–∫–∞: {x_train.shape[0]} samples")
    print(f"   –¢–µ—Å—Ç–æ–≤–∞—è –≤—ã–±–æ—Ä–∫–∞: {x_test.shape[0]} samples")

    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ
    print(f"\nüìã –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ –≤ —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–µ:")
    test_distribution = y_test.value_counts().sort_index()
    for class_label, count in test_distribution.items():
        print(f"   {class_label}: {count} samples")

    # Decision Tree
    dt_model = DecisionTreeClassifier(
        criterion='entropy',
        random_state=42,
        max_depth=10
    )

    # Random Forest
    rf_model = RandomForestClassifier(
        n_estimators=100,
        random_state=42,
        n_jobs=-1,
        max_depth=10
    )

    print("\n" + "üîÑ –û–ë–£–ß–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô" + "üîÅ" * 20)

    # –û–±—É—á–∞–µ–º –º–æ–¥–µ–ª–∏
    dt_model.fit(x_train, y_train)
    rf_model.fit(x_train, y_train)

    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è
    dt_pred = dt_model.predict(x_test)
    rf_pred = rf_model.predict(x_test)

    # –ú–µ—Ç—Ä–∏–∫–∏
    dt_acc = accuracy_score(y_test, dt_pred)
    rf_acc = accuracy_score(y_test, rf_pred)

    print("\n" + "=" * 60)
    print("üìà –°–†–ê–í–ù–ï–ù–ò–ï –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print("=" * 60)

    print(f"üå≥ Decision Tree Accuracy:  {dt_acc:.4f} ({dt_acc * 100:.2f}%)")
    print(f"üå≤ Random Forest Accuracy: {rf_acc:.4f} ({rf_acc * 100:.2f}%)")

    # –ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–∏–π –≤ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è—Ö
    different_predictions = sum(dt_pred != rf_pred)
    print(f"üîÄ –†–∞–∑–ª–∏—á–∞—é—â–∏—Ö—Å—è –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {different_predictions}/{len(y_test)}")

    # –í–∞–∂–Ω–æ—Å—Ç—å –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (—Ç–µ–∫—Å—Ç–æ–≤—ã–π –≤—ã–≤–æ–¥)
    print(f"\nüîù –¢–æ–ø-5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ (Random Forest):")
    feature_importances = pd.DataFrame({
        'feature': x.columns,
        'importance': rf_model.feature_importances_
    }).sort_values('importance', ascending=False)

    for i, row in feature_importances.head().iterrows():
        print(f"   {row['feature']}: {row['importance']:.3f}")

    return dt_model, rf_model, x_train, x_test, y_train, y_test, dt_pred, rf_pred


def analyze_class_performance(y_test, dt_pred, rf_pred, class_names):
    """–ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∫–ª–∞—Å—Å–∞–º"""
    print("\n" + "=" * 60)
    print("üéØ –ê–ù–ê–õ–ò–ó –ü–û –ö–õ–ê–°–°–ê–ú")
    print("=" * 60)

    for class_name in class_names:
        class_mask = y_test == class_name
        if sum(class_mask) > 0:
            dt_correct = sum((y_test[class_mask] == dt_pred[class_mask]))
            rf_correct = sum((y_test[class_mask] == rf_pred[class_mask]))
            total = sum(class_mask)

            print(f"\n{class_name}: {total} samples")
            print(f"   Decision Tree: {dt_correct}/{total} correct ({dt_correct / total * 100:.1f}%)")
            print(f"   Random Forest: {rf_correct}/{total} correct ({rf_correct / total * 100:.1f}%)")


def print_confusion_analysis(y_test, predictions, model_name):
    """–¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –º–∞—Ç—Ä–∏—Ü—ã –æ—à–∏–±–æ–∫"""
    cm = confusion_matrix(y_test, predictions)
    classes = sorted(y_test.unique())

    print(f"\nüîç {model_name} - –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫:")
    for i, true_class in enumerate(classes):
        total = cm[i].sum()
        correct = cm[i][i]
        errors = total - correct

        if errors > 0:
            error_details = []
            for j, pred_class in enumerate(classes):
                if i != j and cm[i][j] > 0:
                    error_details.append(f"{pred_class}({cm[i][j]})")

            if error_details:
                print(f"   {true_class}: {errors} –æ—à–∏–±–æ–∫ ‚Üí {', '.join(error_details)}")


# --- –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ ---
def load_zoo_data():
    query = "SELECT * FROM zoo_training_data;"
    df = pd.read_sql(query, engine)
    print(f"üì¶ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df)} —Å—Ç—Ä–æ–∫ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã zoo_training_data")
    print(f"üìä –ö–æ–ª–æ–Ω–∫–∏: {list(df.columns)}")

    # –£–¥–∞–ª—è–µ–º –Ω–µ—á–∏—Å–ª–æ–≤—É—é –∫–æ–ª–æ–Ω–∫—É animal_name
    if "animal_name" in df.columns:
        df = df.drop(columns=["animal_name"])

    label_col = "animal_type"
    x = df.drop(columns=[label_col])
    y = df[label_col]

    return x, y


# --- –û—Å–Ω–æ–≤–Ω–æ–π –∑–∞–ø—É—Å–∫ ---
def main():
    print("üöÄ –ó–ê–ü–£–°–ö –ê–ù–ê–õ–ò–ó–ê –ú–û–î–ï–õ–ï–ô –ú–ê–®–ò–ù–ù–û–ì–û –û–ë–£–ß–ï–ù–ò–Ø")
    print("=" * 60)

    # –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    x, y = load_zoo_data()

    # –ê–Ω–∞–ª–∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞
    analyze_dataset(x, y)

    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    dt_model, rf_model, x_train, x_test, y_train, y_test, dt_pred, rf_pred = compare_models(x, y)

    # –î–µ—Ç–∞–ª—å–Ω—ã–µ –æ—Ç—á–µ—Ç—ã
    print("\n" + "=" * 60)
    print("üîç –î–ï–¢–ê–õ–¨–ù–´–ï –û–¢–ß–ï–¢–´ –ö–õ–ê–°–°–ò–§–ò–ö–ê–¶–ò–ò")
    print("=" * 60)

    print("\nDecision Tree:")
    print(classification_report(y_test, dt_pred, zero_division=0))

    print("\nRandom Forest:")
    print(classification_report(y_test, rf_pred, zero_division=0))

    # –ê–Ω–∞–ª–∏–∑ –ø–æ –∫–ª–∞—Å—Å–∞–º
    analyze_class_performance(y_test, dt_pred, rf_pred, sorted(y.unique()))

    # –ê–Ω–∞–ª–∏–∑ –æ—à–∏–±–æ–∫
    print_confusion_analysis(y_test, dt_pred, "Decision Tree")
    print_confusion_analysis(y_test, rf_pred, "Random Forest")

    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    os.makedirs("models", exist_ok=True)
    joblib.dump(dt_model, "models/zoo_decision_tree.pkl")
    joblib.dump(rf_model, "models/zoo_random_forest.pkl")

    print(f"\nüíæ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
    print(f"   ‚Ä¢ Decision Tree: models/zoo_decision_tree.pkl")
    print(f"   ‚Ä¢ Random Forest: models/zoo_random_forest.pkl")

    # –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏
    print("\n" + "=" * 60)
    print("üí° –†–ï–ö–û–ú–ï–ù–î–ê–¶–ò–ò –î–õ–Ø –£–õ–£–ß–®–ï–ù–ò–Ø")
    print("=" * 60)
    print("1. üéØ –†–µ—à–µ–Ω–∏–µ –ø—Ä–æ–±–ª–µ–º—ã —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤:")
    print("   ‚Ä¢ –î–æ–±–∞–≤—å—Ç–µ –±–æ–ª—å—à–µ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è reptile, amphibian")
    print("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ oversampling (SMOTE)")
    print("   ‚Ä¢ –ü—Ä–∏–º–µ–Ω–∏—Ç–µ –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤ (class_weight='balanced')")

    print("\n2. üîß –£–ª—É—á—à–µ–Ω–∏–µ –∫–∞—á–µ—Å—Ç–≤–∞ –¥–∞–Ω–Ω—ã—Ö:")
    print("   ‚Ä¢ –ò—Å—Å–ª–µ–¥—É–π—Ç–µ –∏ —É–¥–∞–ª–∏—Ç–µ –¥—É–±–ª–∏–∫–∞—Ç—ã (42% –¥–∞–Ω–Ω—ã—Ö!)")
    print("   ‚Ä¢ –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ—Å—Ç—å —Ä–∞–∑–º–µ—Ç–∫–∏")

    print("\n3. üöÄ –î–∞–ª—å–Ω–µ–π—à–∏–µ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç—ã:")
    print("   ‚Ä¢ –ü–æ–ø—Ä–æ–±—É–π—Ç–µ Gradient Boosting (XGBoost, LightGBM)")
    print("   ‚Ä¢ –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—é")
    print("   ‚Ä¢ –ü–æ—ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä—É–π—Ç–µ —Å –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏")


if __name__ == "__main__":
    main()
